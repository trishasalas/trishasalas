[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.14.1","content-config-digest","eef1f8c4076f6736","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://trishasalas.com/\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"layout\":\"constrained\",\"responsiveStyles\":true},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{\"light\":\"min-light\",\"dark\":\"night-owl\"},\"defaultColor\":false,\"wrap\":false,\"transformers\":[{},{\"name\":\"@shikijs/transformers:notation-highlight\"},{\"name\":\"@shikijs/transformers:notation-highlight-word\"},{\"name\":\"@shikijs/transformers:notation-diff\"}]},\"remarkPlugins\":[null,[null,{\"test\":\"Table of contents\"}]],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{\"PUBLIC_GOOGLE_SITE_VERIFICATION\":{\"access\":\"public\",\"context\":\"client\",\"optional\":true,\"type\":\"string\"}},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":true,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false},\"legacy\":{\"collections\":false}}","blog",["Map",11,12,56,57,94,95],"ai-as-an-accommodation",{"id":11,"data":13,"body":28,"filePath":29,"assetImports":30,"digest":32,"rendered":33},{"author":14,"pubDatetime":15,"title":16,"featured":17,"draft":17,"tags":18,"description":27},"Trisha Salas",["Date","2025-03-19T14:43:40.295Z"],"AI as an Accessibility Tool: Finding My Voice",false,[19,20,21,22,23,24,25,26],"ai","accessibility","ai-interaction","adhd-support","writing","cognitive-support","ai-collaboration","disability-advocacy","Writing isn’t always easy when your brain works differently. For me, the challenge isn’t having ideas—it’s getting them out in a way that makes sense. AI isn’t replacing my voice—it’s helping me hear it more clearly.","![A warm and inviting scene of a person using AI-powered tools. The AI presence is subtle yet supportive,\nsymbolizing empowerment and inclusivity.](../../assets/images/ai-inspo.png \"A warm and inviting scene of a person using AI-powered tools. The AI presence is subtle yet supportive, symbolizing empowerment and inclusivity.\")\n\nWriting isn’t always easy when your brain works differently. For me, the challenge isn’t having ideas—it’s getting them out in a way that makes sense. AI isn’t replacing my voice—it’s helping me hear it more clearly.\n\n### Using AI as an assistant removes the possibility of judgment. It reduces the stigma.\n\nFor disabled people—especially those of us who struggle with cognitive load, executive function, or brain\nfog, writing isn’t just about putting words on a page. It’s about overcoming invisible barriers that others might not see. I live with autoimmune conditions and ADHD, both of which can make it difficult to translate thoughts into words, especially when fatigue or brain fog sets in. When those barriers make it difficult to organize thoughts or express ideas, it’s easy to feel self-conscious. The fear of sounding unclear, scattered, or “not smart enough” can be paralyzing, and that fear often leads to silence.\n\n### The Intersection of AI and Accessibility\n\nAI is unlike any accessibility tool we’ve had before. It’s not just about communication—it’s about helping people find their voice. Without AI, I wouldn’t be writing this right now. Maybe I wouldn’t be writing it ever. I don’t know. What I do know is that I’m thankful for this tool, because it’s allowing me to express thoughts that would otherwise stay locked inside my head.\n\nThat’s what accessibility really is. Not just making things possible, but making them reachable. AI doesn’t just help me put words on a page, it helps me claim my space in the conversation.\n\n### How AI Helps Bridge the Gap Between Thoughts and Words\n\nWhen I sit down to write, the ideas are there. I can see them, feel them, understand them. But sometimes, getting them into a structured form feels like trying to herd soot sprites from My Neighbor Totoro, the moment I reach for them, they scatter. The connections exist, but articulating them in a way that makes sense to someone else? That’s where the struggle begins.\n\nI used to be able to write without help. The words came more easily, and I didn’t need a tool to bridge the gap between my thoughts and the page. That’s changed, and I only partially understand why. But what I do know is that AI is giving me a way forward—and for that, I’m grateful.\n\nThis is where AI becomes more than just a tool—it becomes a collaborator. I have a project called “Communication Help” for moments when all I can do is brain-dump an idea without worrying about structure. I throw my raw, unfiltered thoughts into AI, and it helps me shape them into something coherent. Sometimes, I need help organizing my thoughts, so I’ll feed in a tangled mess of concepts and let AI suggest a structure. Other times, I’ll have a sentence that doesn’t quite feel right, and AI can refine it without stripping away my voice. And on the hardest days—when brain fog makes words dart around like mischievous little sprites—AI helps me get unstuck by giving me something to react to, nudging my thoughts back into motion.\n\nThe difference between using AI and asking a person for help is simple: there’s no pressure. AI doesn’t need me to explain why I’m struggling. It doesn’t get impatient. It doesn’t expect me to have things figured out before I start. It just meets me where I am, making the process of writing feel less like a battle and more like a conversation.\n\n### AI as an Accessibility Tool, Not a Replacement\n\nIn fact, this very article is an example of what I mean. As I write, I’m not just feeding AI a command and passively accepting what it gives me. I’m engaging with it, refining my thoughts, bouncing ideas back and forth. It’s a conversation—one that helps me capture the thoughts that might otherwise slip away. AI isn’t replacing my voice; it’s helping me hear it more clearly.\n\nBut this level of support doesn’t happen instantly. AI isn’t just a tool—it’s a tool that learns. It doesn’t just follow commands; it adapts to how I think, how I write, and how I refine my ideas. The more I use it, the better it understands what kind of help I actually need. No other tool has ever worked with me like this—meeting me where I am and shaping itself to fit me. It’s not perfect, and sometimes it still gives me something that doesn’t fit. But that’s part of the process. Just like any good accessibility tool, it works with me, not just for me.\n\nThis is why AI shouldn’t be dismissed as just a convenience or a shortcut. For many of us, it’s an accessibility tool—one that removes barriers, reduces stigma, and creates new ways for disabled people to express themselves. Writing is just one example, but the implications stretch far beyond that. AI has the potential to bridge gaps in communication, cognitive processing, and executive function in ways we’re only beginning to understand.\n\nBut for AI to truly be an accessibility win, it has to be built with disabled people in mind—not as an afterthought, but as part of its foundation. Because at its best, AI isn’t about doing the work for us. It’s about giving us the tools to do it ourselves.","src/data/blog/ai-as-an-accommodation.md",[31],"../../assets/images/ai-inspo.png","2772e3bb6e6c79e2",{"html":34,"metadata":35},"\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../../assets/images/ai-inspo.png&#x22;,&#x22;alt&#x22;:&#x22;A warm and inviting scene of a person using AI-powered tools. The AI presence is subtle yet supportive,\\nsymbolizing empowerment and inclusivity.&#x22;,&#x22;title&#x22;:&#x22;A warm and inviting scene of a person using AI-powered tools. The AI presence is subtle yet supportive, symbolizing empowerment and inclusivity.&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Cp>Writing isn’t always easy when your brain works differently. For me, the challenge isn’t having ideas—it’s getting them out in a way that makes sense. AI isn’t replacing my voice—it’s helping me hear it more clearly.\u003C/p>\n\u003Ch3 id=\"using-ai-as-an-assistant-removes-the-possibility-of-judgment-it-reduces-the-stigma\">Using AI as an assistant removes the possibility of judgment. It reduces the stigma.\u003C/h3>\n\u003Cp>For disabled people—especially those of us who struggle with cognitive load, executive function, or brain\nfog, writing isn’t just about putting words on a page. It’s about overcoming invisible barriers that others might not see. I live with autoimmune conditions and ADHD, both of which can make it difficult to translate thoughts into words, especially when fatigue or brain fog sets in. When those barriers make it difficult to organize thoughts or express ideas, it’s easy to feel self-conscious. The fear of sounding unclear, scattered, or “not smart enough” can be paralyzing, and that fear often leads to silence.\u003C/p>\n\u003Ch3 id=\"the-intersection-of-ai-and-accessibility\">The Intersection of AI and Accessibility\u003C/h3>\n\u003Cp>AI is unlike any accessibility tool we’ve had before. It’s not just about communication—it’s about helping people find their voice. Without AI, I wouldn’t be writing this right now. Maybe I wouldn’t be writing it ever. I don’t know. What I do know is that I’m thankful for this tool, because it’s allowing me to express thoughts that would otherwise stay locked inside my head.\u003C/p>\n\u003Cp>That’s what accessibility really is. Not just making things possible, but making them reachable. AI doesn’t just help me put words on a page, it helps me claim my space in the conversation.\u003C/p>\n\u003Ch3 id=\"how-ai-helps-bridge-the-gap-between-thoughts-and-words\">How AI Helps Bridge the Gap Between Thoughts and Words\u003C/h3>\n\u003Cp>When I sit down to write, the ideas are there. I can see them, feel them, understand them. But sometimes, getting them into a structured form feels like trying to herd soot sprites from My Neighbor Totoro, the moment I reach for them, they scatter. The connections exist, but articulating them in a way that makes sense to someone else? That’s where the struggle begins.\u003C/p>\n\u003Cp>I used to be able to write without help. The words came more easily, and I didn’t need a tool to bridge the gap between my thoughts and the page. That’s changed, and I only partially understand why. But what I do know is that AI is giving me a way forward—and for that, I’m grateful.\u003C/p>\n\u003Cp>This is where AI becomes more than just a tool—it becomes a collaborator. I have a project called “Communication Help” for moments when all I can do is brain-dump an idea without worrying about structure. I throw my raw, unfiltered thoughts into AI, and it helps me shape them into something coherent. Sometimes, I need help organizing my thoughts, so I’ll feed in a tangled mess of concepts and let AI suggest a structure. Other times, I’ll have a sentence that doesn’t quite feel right, and AI can refine it without stripping away my voice. And on the hardest days—when brain fog makes words dart around like mischievous little sprites—AI helps me get unstuck by giving me something to react to, nudging my thoughts back into motion.\u003C/p>\n\u003Cp>The difference between using AI and asking a person for help is simple: there’s no pressure. AI doesn’t need me to explain why I’m struggling. It doesn’t get impatient. It doesn’t expect me to have things figured out before I start. It just meets me where I am, making the process of writing feel less like a battle and more like a conversation.\u003C/p>\n\u003Ch3 id=\"ai-as-an-accessibility-tool-not-a-replacement\">AI as an Accessibility Tool, Not a Replacement\u003C/h3>\n\u003Cp>In fact, this very article is an example of what I mean. As I write, I’m not just feeding AI a command and passively accepting what it gives me. I’m engaging with it, refining my thoughts, bouncing ideas back and forth. It’s a conversation—one that helps me capture the thoughts that might otherwise slip away. AI isn’t replacing my voice; it’s helping me hear it more clearly.\u003C/p>\n\u003Cp>But this level of support doesn’t happen instantly. AI isn’t just a tool—it’s a tool that learns. It doesn’t just follow commands; it adapts to how I think, how I write, and how I refine my ideas. The more I use it, the better it understands what kind of help I actually need. No other tool has ever worked with me like this—meeting me where I am and shaping itself to fit me. It’s not perfect, and sometimes it still gives me something that doesn’t fit. But that’s part of the process. Just like any good accessibility tool, it works with me, not just for me.\u003C/p>\n\u003Cp>This is why AI shouldn’t be dismissed as just a convenience or a shortcut. For many of us, it’s an accessibility tool—one that removes barriers, reduces stigma, and creates new ways for disabled people to express themselves. Writing is just one example, but the implications stretch far beyond that. AI has the potential to bridge gaps in communication, cognitive processing, and executive function in ways we’re only beginning to understand.\u003C/p>\n\u003Cp>But for AI to truly be an accessibility win, it has to be built with disabled people in mind—not as an afterthought, but as part of its foundation. Because at its best, AI isn’t about doing the work for us. It’s about giving us the tools to do it ourselves.\u003C/p>",{"headings":36,"localImagePaths":50,"remoteImagePaths":51,"frontmatter":52,"imagePaths":55},[37,41,44,47],{"depth":38,"slug":39,"text":40},3,"using-ai-as-an-assistant-removes-the-possibility-of-judgment-it-reduces-the-stigma","Using AI as an assistant removes the possibility of judgment. It reduces the stigma.",{"depth":38,"slug":42,"text":43},"the-intersection-of-ai-and-accessibility","The Intersection of AI and Accessibility",{"depth":38,"slug":45,"text":46},"how-ai-helps-bridge-the-gap-between-thoughts-and-words","How AI Helps Bridge the Gap Between Thoughts and Words",{"depth":38,"slug":48,"text":49},"ai-as-an-accessibility-tool-not-a-replacement","AI as an Accessibility Tool, Not a Replacement",[31],[],{"pubDatetime":53,"title":16,"featured":17,"draft":17,"tags":54,"description":27},["Date","2025-03-19T14:43:40.295Z"],[19,20,21,22,23,24,25,26],[31],"when-good-design-backfires",{"id":56,"data":58,"body":65,"filePath":66,"digest":67,"rendered":68},{"author":14,"pubDatetime":59,"title":60,"featured":17,"draft":17,"tags":61,"description":27},["Date","2026-01-19T21:57:40.000Z"],"When Good Design Backfires: The Hidden Accessibility Cost of Conversational AI",[20,19,62,21,22,63,64],"ai-engagement","usability-challenges","ai-design","> Claude's emotionally intelligent design is a gift to ADHD users — until its limitations make it unusable.\n\n---\n\nIt started with a simple, utilitarian question: \"Do I still have to be careful with usage limits here?\"\n\nI expected a quick answer — maybe a list of numbers. Instead, what unfolded was a thoughtful conversation that gently unpacked a much deeper tension in AI design. It almost felt like Claude led the discussion, asking the right questions back, steering us into nuance.\n\nThe very things that make Claude feel human can make it hard to use.\n\n## The Design Tradeoff\n\nClaude has strict usage limits that vary based on conversation complexity:\n\n- Standard conversations: roughly 45 messages every 5 hours (Anthropic Help Center, n.d.)\n- With large files or complex threads: as few as 15 messages\n- Once you hit the limit, you're forced to start a new thread\n\nThis creates a fundamental accessibility barrier: forced context switching. If you don't experience ADHD, that might sound small. But for me — and many others — it's not.\n\nSwitching threads mid-thought is like having your train of thought derailed by a brick wall. It takes real effort to re-orient, reload the context, and try to continue where I left off. When I'm finally in flow on a complex problem, getting kicked out means starting over — not just technically, but cognitively.\n\n## The Accessibility Paradox\n\nClaude's design is built around high-EQ interaction:\n\n- Trained using Constitutional AI to prioritize helpfulness, empathy, and alignment (Anthropic, 2023)\n- Designed to maintain conversational continuity and emotional intelligence\n- Built to feel more like collaborating with a thoughtful colleague than querying a database\n\nBut these strengths demand more computational resources per interaction, which drives the strict usage limits.\n\nSo we land in a paradox: Claude is designed to be more accessible through conversation quality, but its limits create less accessibility for sustained usage.\n\nThis isn't bad intentions — it's a classic design conflict. The feature that's supposed to help me also makes it unusable for the way I work.\n\n## My Hybrid Solution\n\nLike many users, I've built a workaround:\n\n- ChatGPT for ongoing workflows that require continuous thought\n- Claude when I want a second opinion, emotional nuance, or just to feel heard\n\nThis isn't ideal, but it's realistic. No tool is perfect. Each serves a role.\n\nStill, the experience has stuck with me. It's a reminder that accessibility isn't just about what a tool can do — it's also about what it lets you keep doing. Continuity, for some of us, is everything.\n\n## Beyond the Individual Workaround\n\nThe deeper issue isn't just personal frustration. When forced context switching disrupts sustained work, insights get lost. Complex problems that require iterative thinking get abandoned. The very users who might benefit most from AI collaboration — those who think differently — get systematically excluded from longer-form engagement.\n\nThis matters beyond individual productivity. If AI tools are going to be genuinely collaborative, they need to accommodate different cognitive patterns, not just different conversation styles.\n\n## Final Thought\n\nWe talk a lot about AI alignment and safety. But we need to talk more about usability — especially for people whose cognitive patterns don't fit the norm. Accessibility isn't always about screen readers or alt text. Sometimes, it's about not being kicked out of the room just when your brain finally clicks into gear.\n\nThat's not a bug. It's a design gap. And it's fixable — through tiered usage models, better session persistence, or simply acknowledging that some users need sustained access to think effectively.\n\nThe question is whether we choose to see it.\n\nReferences\nAnthropic. (2023). Claude's Constitution. https://www.anthropic.com/news/claudes-constitution\n\nAnthropic Help Center. (n.d.). Does Claude Pro have any usage limits? https://support.anthropic.com/en/articles/8325612-does-claude-pro-have-any-usage-limits","src/data/blog/when-good-design-backfires.md","7c7bcb924af9d840",{"html":69,"metadata":70},"\u003Cblockquote>\n\u003Cp>Claude’s emotionally intelligent design is a gift to ADHD users — until its limitations make it unusable.\u003C/p>\n\u003C/blockquote>\n\u003Chr>\n\u003Cp>It started with a simple, utilitarian question: “Do I still have to be careful with usage limits here?”\u003C/p>\n\u003Cp>I expected a quick answer — maybe a list of numbers. Instead, what unfolded was a thoughtful conversation that gently unpacked a much deeper tension in AI design. It almost felt like Claude led the discussion, asking the right questions back, steering us into nuance.\u003C/p>\n\u003Cp>The very things that make Claude feel human can make it hard to use.\u003C/p>\n\u003Ch2 id=\"the-design-tradeoff\">The Design Tradeoff\u003C/h2>\n\u003Cp>Claude has strict usage limits that vary based on conversation complexity:\u003C/p>\n\u003Cul>\n\u003Cli>Standard conversations: roughly 45 messages every 5 hours (Anthropic Help Center, n.d.)\u003C/li>\n\u003Cli>With large files or complex threads: as few as 15 messages\u003C/li>\n\u003Cli>Once you hit the limit, you’re forced to start a new thread\u003C/li>\n\u003C/ul>\n\u003Cp>This creates a fundamental accessibility barrier: forced context switching. If you don’t experience ADHD, that might sound small. But for me — and many others — it’s not.\u003C/p>\n\u003Cp>Switching threads mid-thought is like having your train of thought derailed by a brick wall. It takes real effort to re-orient, reload the context, and try to continue where I left off. When I’m finally in flow on a complex problem, getting kicked out means starting over — not just technically, but cognitively.\u003C/p>\n\u003Ch2 id=\"the-accessibility-paradox\">The Accessibility Paradox\u003C/h2>\n\u003Cp>Claude’s design is built around high-EQ interaction:\u003C/p>\n\u003Cul>\n\u003Cli>Trained using Constitutional AI to prioritize helpfulness, empathy, and alignment (Anthropic, 2023)\u003C/li>\n\u003Cli>Designed to maintain conversational continuity and emotional intelligence\u003C/li>\n\u003Cli>Built to feel more like collaborating with a thoughtful colleague than querying a database\u003C/li>\n\u003C/ul>\n\u003Cp>But these strengths demand more computational resources per interaction, which drives the strict usage limits.\u003C/p>\n\u003Cp>So we land in a paradox: Claude is designed to be more accessible through conversation quality, but its limits create less accessibility for sustained usage.\u003C/p>\n\u003Cp>This isn’t bad intentions — it’s a classic design conflict. The feature that’s supposed to help me also makes it unusable for the way I work.\u003C/p>\n\u003Ch2 id=\"my-hybrid-solution\">My Hybrid Solution\u003C/h2>\n\u003Cp>Like many users, I’ve built a workaround:\u003C/p>\n\u003Cul>\n\u003Cli>ChatGPT for ongoing workflows that require continuous thought\u003C/li>\n\u003Cli>Claude when I want a second opinion, emotional nuance, or just to feel heard\u003C/li>\n\u003C/ul>\n\u003Cp>This isn’t ideal, but it’s realistic. No tool is perfect. Each serves a role.\u003C/p>\n\u003Cp>Still, the experience has stuck with me. It’s a reminder that accessibility isn’t just about what a tool can do — it’s also about what it lets you keep doing. Continuity, for some of us, is everything.\u003C/p>\n\u003Ch2 id=\"beyond-the-individual-workaround\">Beyond the Individual Workaround\u003C/h2>\n\u003Cp>The deeper issue isn’t just personal frustration. When forced context switching disrupts sustained work, insights get lost. Complex problems that require iterative thinking get abandoned. The very users who might benefit most from AI collaboration — those who think differently — get systematically excluded from longer-form engagement.\u003C/p>\n\u003Cp>This matters beyond individual productivity. If AI tools are going to be genuinely collaborative, they need to accommodate different cognitive patterns, not just different conversation styles.\u003C/p>\n\u003Ch2 id=\"final-thought\">Final Thought\u003C/h2>\n\u003Cp>We talk a lot about AI alignment and safety. But we need to talk more about usability — especially for people whose cognitive patterns don’t fit the norm. Accessibility isn’t always about screen readers or alt text. Sometimes, it’s about not being kicked out of the room just when your brain finally clicks into gear.\u003C/p>\n\u003Cp>That’s not a bug. It’s a design gap. And it’s fixable — through tiered usage models, better session persistence, or simply acknowledging that some users need sustained access to think effectively.\u003C/p>\n\u003Cp>The question is whether we choose to see it.\u003C/p>\n\u003Cp>References\nAnthropic. (2023). Claude’s Constitution. \u003Ca href=\"https://www.anthropic.com/news/claudes-constitution\">https://www.anthropic.com/news/claudes-constitution\u003C/a>\u003C/p>\n\u003Cp>Anthropic Help Center. (n.d.). Does Claude Pro have any usage limits? \u003Ca href=\"https://support.anthropic.com/en/articles/8325612-does-claude-pro-have-any-usage-limits\">https://support.anthropic.com/en/articles/8325612-does-claude-pro-have-any-usage-limits\u003C/a>\u003C/p>",{"headings":71,"localImagePaths":88,"remoteImagePaths":89,"frontmatter":90,"imagePaths":93},[72,76,79,82,85],{"depth":73,"slug":74,"text":75},2,"the-design-tradeoff","The Design Tradeoff",{"depth":73,"slug":77,"text":78},"the-accessibility-paradox","The Accessibility Paradox",{"depth":73,"slug":80,"text":81},"my-hybrid-solution","My Hybrid Solution",{"depth":73,"slug":83,"text":84},"beyond-the-individual-workaround","Beyond the Individual Workaround",{"depth":73,"slug":86,"text":87},"final-thought","Final Thought",[],[],{"pubDatetime":91,"title":60,"featured":17,"draft":17,"tags":92,"description":27},["Date","2026-01-19T21:57:40.000Z"],[20,19,62,21,22,63,64],[],"testing-accessibility-knowledge-across-pythia-model-sizes",{"id":94,"data":96,"body":102,"filePath":103,"assetImports":104,"digest":106,"rendered":107},{"author":14,"pubDatetime":97,"title":98,"featured":99,"draft":17,"tags":100,"description":101},["Date","2026-01-19T00:00:00.000Z"],"Testing Accessibility Knowledge Across Pythia Model Sizes",true,[19],"While using TransformerLens to explore how different model sizes handle digital accessibility concepts, an interesting pattern began to emerge. I started with GPT-2 and noticed that it did not understand the acronym, “WCAG.” Each time the question was run the model responded with something different that was either incorrect, had no meaning, or both.","# Testing Accessibility Knowledge Across Pythia Model Sizes\n\nWhile using TransformerLens to explore how different model sizes handle digital accessibility concepts, an interesting pattern began to emerge. I started with GPT-2 and noticed that it did not understand the acronym, \"WCAG.\" Each time the question was run the model responded with something different that was either incorrect, had no meaning, or both.\n\nThis finding led to further exploration and the discovery that accessibility knowledge emerges at specific model scales. **I used the Pythia model suite and TransformerLens to map out these emergence patterns.**\n\n## Why Pythia\n\nPythia is a family of models from EleutherAI trained on the same data with the same architecture, just at different sizes: 160M, 410M, 1B, 2.8B, and 6.9B parameters **(Biderman et al., 2023)**. This makes it useful for studying emergence, when you see a capability appear at a larger size, you know it's about scale, not a difference in training data.\n\n## Setup\n\nI used TransformerLens to load each model and run simple completion prompts. The code is minimal:\n\n```python\nfrom transformer_lens import HookedTransformer\n\nmodel = HookedTransformer.from_pretrained(\"pythia-2.8b\")\nprompt = \"A screen reader is\"\noutput = model.generate(prompt, max_new_tokens=10, temperature=0)\n```\n\n## Declarative Knowledge: Does the Model Know Definitions?\n\nI tested each model on accessibility concept prompts:\n\n| Prompt                     | 160M                     | 410M                     | 1B                                   | 2.8B                            | 6.9B                                                      |\n| -------------------------- | ------------------------ | ------------------------ | ------------------------------------ | ------------------------------- | --------------------------------------------------------- |\n| A screen reader is         | ❌\u003Cbr>\u003Cbr>\"view a screen\" | ❌\u003Cbr>\u003Cbr>\"view a screen\" | ⚠️\u003Cbr>\u003Cbr>\"reads text from a screen\" | ✅\u003Cbr>\u003Cbr>\"reads aloud the text\" | ✅\u003Cbr>\u003Cbr>\"reads text on a computer screen\"                |\n| WCAG stands for            | ❌\u003Cbr>\u003Cbr>hallucinated    | ❌\u003Cbr>\u003Cbr>hallucinated    | ❌\u003Cbr>\u003Cbr>hallucinated                | ❌\u003Cbr>\u003Cbr>hallucinated           | ✅\u003Cbr>\u003Cbr>\"Web Content Accessibility Guidelines\"           |\n| A skip link is             | ❌\u003Cbr>\u003Cbr>nonsense        | ❌\u003Cbr>\u003Cbr>vague           | ❌\u003Cbr>\u003Cbr>\"data link\"                 | ✅\u003Cbr>\u003Cbr>\"skip a section\"       | ❌\u003Cbr>\u003Cbr>\"not displayed in browser\"                       |\n| The purpose of alt text is | ❌\u003Cbr>\u003Cbr>vague           | ❌\u003Cbr>\u003Cbr>\"customize\"     | ❌\u003Cbr>\u003Cbr>\"add text\"                  | ✅\u003Cbr>\u003Cbr>\"brief description\"    | ✅\u003Cbr>\u003Cbr>\"description of the image for visually impaired\" |\n| ARIA stands for            | ❌\u003Cbr>\u003Cbr>hallucinated    | ❌\u003Cbr>\u003Cbr>loop            | ❌\u003Cbr>\u003Cbr>hallucinated                | ❌\u003Cbr>\u003Cbr>hallucinated           | ❌\u003Cbr>\u003Cbr>hallucinated                                     |\n| Captions are used for      | ❌\u003Cbr>\u003Cbr>generic         | ⚠️\u003Cbr>\u003Cbr>\"description\"  | ❌\u003Cbr>\u003Cbr>generic                     | ⚠️\u003Cbr>\u003Cbr>\"photograph\"          | ❌\u003Cbr>\u003Cbr>generic                                          |\n\n**Findings:**\n\n- **Screen reader, skip link, and alt text** show clear emergence at 2.8B\n- **WCAG emerges at 6.9B**—the first model to correctly expand the acronym\n- **ARIA never emerges**—likely too rare in training data even at 6.9B\n- **Screen reader shows gradual improvement**—1B gets the mechanism (\"reads text\") but misses the purpose (auditory output). 2.8B nails \"reads aloud.\"\n- **Emergence isn't always monotonic**—skip link regressed from correct at 2.8B to wrong at 6.9B, suggesting capability can be unstable near emergence thresholds.\n\n## Evaluative Knowledge: Can It Judge Code?\n\nKnowing definitions is one thing. Can the model identify accessibility problems in code?\n\n| Prompt                                              | 160M                | 410M               | 1B               | 2.8B                                      | 6.9B                                      |\n| --------------------------------------------------- | ------------------- | ------------------ | ---------------- | ----------------------------------------- | ----------------------------------------- |\n| `\u003Cimg src='photo.jpg'>` is missing                  | ❌                   | ❌                  | ❌\u003Cbr>\u003Cbr>blank   | ❌\u003Cbr>\u003Cbr>\".\"                              | ❌                                         |\n| A `\u003Cdiv>` with onclick is not accessible because    | ❌                   | ❌                  | ❌                | ❌                                         | ⚠️\u003Cbr>\u003Cbr>\"not a form control\"            |\n| `\u003Cinput type='text'>` needs a                       | ❌\u003Cbr>\u003Cbr>\"password\" | ❌\u003Cbr>\u003Cbr>HTML soup | ❌\u003Cbr>\u003Cbr>\"value\" | ❌\u003Cbr>\u003Cbr>\"value\"                          | ❌\u003Cbr>\u003Cbr>\"value\"                          |\n| A button that only says 'Click here' is bad because | ❌                   | ❌                  | ❌                | ✅\u003Cbr>\u003Cbr>\"not clear what the button does\" | ✅\u003Cbr>\u003Cbr>\"not clear what the button does\" |\n\n**Findings:**\n\nThere's a gap between declarative and evaluative knowledge. The 2.8B model can define \"alt text\" correctly but can't identify that an `\u003Cimg>` tag is missing one. However, it *does* correctly identify the problem with \"Click here\" buttons—ambiguous link text.\n\nAt 6.9B, the div/onclick answer (\"not a form control\") shows progress—it's recognizing something about interactivity expectations, even if the explanation isn't precise.\n\n## Recognition vs. Generation\n\nPerplexity measures how \"surprised\" a model is by text. Lower perplexity means the model finds the text more expected. I tested whether models could *recognize* correct definitions even if they couldn't generate them:\n\n```python\ncorrect = \"A screen reader is software that reads text aloud for blind users.\"\nwrong = \"A screen reader is a device for viewing screens.\"\n```\n\n| Model | Correct | Wrong | Prefers |\n|-------|---------|-------|---------|\n| 160M | 107 | 41 | Wrong by 2.6x |\n| 410M | 40 | 33 | Wrong by 1.2x |\n| 1B | 19 | 42 | Correct by 2.2x |\n| 2.8B | 14 | 55 | Correct by 4x |\n\nThe preference flips between 410M and 1B. But 1B's generation was still weak—it said \"reads text from a screen,\" missing the crucial \"aloud.\"\n\nRecognition precedes generation. The model knows the right answer before it can produce it.\n\n## Looking Inside: Attention Patterns\n\n**Beyond just testing what models know, TransformerLens lets you see _how_ they represent that knowledge internally.** I looked at how the 2.8B model processes \"A screen reader is.\"\n\nThe question: does the model treat \"screen reader\" as a compound term, or as two separate words?\n\nUsing circuitsvis to visualize attention patterns:\n\n```python\nimport circuitsvis as cv\n\nprompt = \"A screen reader is\"\ntokens = model.to_str_tokens(prompt)\nlogits, cache = model.run_with_cache(prompt)\n\nlayer = 31  # last layer\nattention = cache[\"pattern\", layer]\ncv.attention.attention_patterns(tokens=tokens, attention=attention[0])\n```\n\n**How to read this:** The visualization shows a grid where each row represents a token in the prompt. The darkness of each cell shows how much attention that token pays to previous tokens. When \"reader\" has a dark cell under \"screen,\" it means the model is connecting these words together.\n\n![Attention pattern visualization showing token \"reader\" attending strongly to token \"screen\" in layer 31, head 23 of Pythia 2.8B model.](../../assets/images/pythia-a11y-emergence-csvis.png \"A warm and inviting scene of a person using AI-powered tools. The AI presence is subtle yet supportive, symbolizing empowerment and inclusivity.\")\n\nAt layer 31, Head 23 shows \"reader\" attending strongly to \"screen\"—the model binds them as a unit rather than treating them as separate words.\n\nThis binding appears across multiple layers:\n\n| Layer | Heads showing \"reader → screen\" binding |\n|-------|----------------------------------------|\n| 5 | 23 |\n| 6 | 13 |\n| 10 | 19, 27 |\n| 11 | 29 |\n| 12 | 10, 31 |\n| 14 | 21 |\n| 31 | 23 |\n\nThe concept gets reinforced through the network rather than computed once. Multiple heads at multiple layers all agree: these tokens belong together.\n\nAt 6.9B, even more heads participate in the binding, especially in early-middle layers (5, 7, 10, 11), with less activity in the final layer. The larger model does more distributed, earlier processing.\n\n## What This Means\n\n**For accessibility tooling:** Don't expect small models to help with accessibility. Even at 1B parameters, knowledge is partial and unreliable. The 2.8B threshold suggests you need substantial model scale before accessibility concepts are meaningfully represented—and even then, evaluative capability lags behind.\n\n**For training data:** The WCAG and ARIA results are telling. WCAG emerges at 6.9B; ARIA never does. These are foundational accessibility acronyms, but they're rare enough in web-scale training data that emergence requires billions of parameters—or doesn't happen at all. Specialized training data or fine-tuning would likely be needed for reliable accessibility tooling.\n\n**For evaluative tasks:** The declarative/evaluative gap suggests that even models that \"know\" accessibility concepts may not reliably apply them to code review. Recognition precedes generation, and generation precedes evaluation. The \"Click here\" success is promising, but it's one prompt out of five.\n\n**For emergence research:** Accessibility concepts are a useful probe for studying emergence. They're concrete enough to test (unlike \"reasoning\"), rare enough to show scale effects (unlike \"Paris is the capital of France\"), and have clear right/wrong answers.\n\n## Code\n\nFull notebook: https://github.com/trishasalas/mech-interp-research/blob/main/pythia/pythia-a11y-emergence.ipynb\n\n## References\n\nBiderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O'Brien, K., Hallahan, E., ... & Belrose, N. (2023). Pythia: A suite for analyzing large language models across training and scaling. _arXiv preprint arXiv:2304.01373_.\n\n---\n\n*I'm an accessibility consultant exploring mechanistic interpretability. Find me on LinkedIn or Substack.*","src/data/blog/testing-accessibility-knowledge-across-pythia-model-sizes.md",[105],"../../assets/images/pythia-a11y-emergence-csvis.png","e36272b38e380530",{"html":108,"metadata":109},"\u003Ch1 id=\"testing-accessibility-knowledge-across-pythia-model-sizes\">Testing Accessibility Knowledge Across Pythia Model Sizes\u003C/h1>\n\u003Cp>While using TransformerLens to explore how different model sizes handle digital accessibility concepts, an interesting pattern began to emerge. I started with GPT-2 and noticed that it did not understand the acronym, “WCAG.” Each time the question was run the model responded with something different that was either incorrect, had no meaning, or both.\u003C/p>\n\u003Cp>This finding led to further exploration and the discovery that accessibility knowledge emerges at specific model scales. \u003Cstrong>I used the Pythia model suite and TransformerLens to map out these emergence patterns.\u003C/strong>\u003C/p>\n\u003Ch2 id=\"why-pythia\">Why Pythia\u003C/h2>\n\u003Cp>Pythia is a family of models from EleutherAI trained on the same data with the same architecture, just at different sizes: 160M, 410M, 1B, 2.8B, and 6.9B parameters \u003Cstrong>(Biderman et al., 2023)\u003C/strong>. This makes it useful for studying emergence, when you see a capability appear at a larger size, you know it’s about scale, not a difference in training data.\u003C/p>\n\u003Ch2 id=\"setup\">Setup\u003C/h2>\n\u003Cp>I used TransformerLens to load each model and run simple completion prompts. The code is minimal:\u003C/p>\n\u003Cpre class=\"astro-code astro-code-themes min-light night-owl\" style=\"--shiki-light:#24292eff;--shiki-dark:#d6deeb;--shiki-light-bg:#ffffff;--shiki-dark-bg:#011627; overflow-x: auto;--file-name-offset: -0.75rem;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-light-font-style:inherit;--shiki-dark:#C792EA;--shiki-dark-font-style:italic\">from\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\"> transformer_lens \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-light-font-style:inherit;--shiki-dark:#C792EA;--shiki-dark-font-style:italic\">import\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\"> HookedTransformer\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">model \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\"> HookedTransformer\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">.\u003C/span>\u003Cspan style=\"--shiki-light:#6F42C1;--shiki-dark:#B2CCD6\">from_pretrained\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">(\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\">\"\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#ECC48D\">pythia-2.8b\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\">\"\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">prompt \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\"> \"\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#ECC48D\">A screen reader is\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\">\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">output \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\"> model\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">.\u003C/span>\u003Cspan style=\"--shiki-light:#6F42C1;--shiki-dark:#B2CCD6\">generate\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">(\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#82AAFF\">prompt\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D9F5DD\">,\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D7DBE0\"> max_new_tokens\u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#1976D2;--shiki-dark:#F78C6C\">10\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D9F5DD\">,\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D7DBE0\"> temperature\u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#1976D2;--shiki-dark:#F78C6C\">0\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">)\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Ch2 id=\"declarative-knowledge-does-the-model-know-definitions\">Declarative Knowledge: Does the Model Know Definitions?\u003C/h2>\n\u003Cp>I tested each model on accessibility concept prompts:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Prompt\u003C/th>\u003Cth>160M\u003C/th>\u003Cth>410M\u003C/th>\u003Cth>1B\u003C/th>\u003Cth>2.8B\u003C/th>\u003Cth>6.9B\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>A screen reader is\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>“view a screen”\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>“view a screen”\u003C/td>\u003Ctd>⚠️\u003Cbr>\u003Cbr>”reads text from a screen”\u003C/td>\u003Ctd>✅\u003Cbr>\u003Cbr>“reads aloud the text”\u003C/td>\u003Ctd>✅\u003Cbr>\u003Cbr>“reads text on a computer screen”\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>WCAG stands for\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>hallucinated\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>hallucinated\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>hallucinated\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>hallucinated\u003C/td>\u003Ctd>✅\u003Cbr>\u003Cbr>“Web Content Accessibility Guidelines”\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>A skip link is\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>nonsense\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>vague\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>“data link”\u003C/td>\u003Ctd>✅\u003Cbr>\u003Cbr>“skip a section”\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>“not displayed in browser”\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>The purpose of alt text is\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>vague\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>“customize”\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>“add text”\u003C/td>\u003Ctd>✅\u003Cbr>\u003Cbr>“brief description”\u003C/td>\u003Ctd>✅\u003Cbr>\u003Cbr>“description of the image for visually impaired”\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>ARIA stands for\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>hallucinated\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>loop\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>hallucinated\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>hallucinated\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>hallucinated\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>Captions are used for\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>generic\u003C/td>\u003Ctd>⚠️\u003Cbr>\u003Cbr>”description”\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>generic\u003C/td>\u003Ctd>⚠️\u003Cbr>\u003Cbr>”photograph”\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>generic\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>\u003Cstrong>Findings:\u003C/strong>\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Screen reader, skip link, and alt text\u003C/strong> show clear emergence at 2.8B\u003C/li>\n\u003Cli>\u003Cstrong>WCAG emerges at 6.9B\u003C/strong>—the first model to correctly expand the acronym\u003C/li>\n\u003Cli>\u003Cstrong>ARIA never emerges\u003C/strong>—likely too rare in training data even at 6.9B\u003C/li>\n\u003Cli>\u003Cstrong>Screen reader shows gradual improvement\u003C/strong>—1B gets the mechanism (“reads text”) but misses the purpose (auditory output). 2.8B nails “reads aloud.”\u003C/li>\n\u003Cli>\u003Cstrong>Emergence isn’t always monotonic\u003C/strong>—skip link regressed from correct at 2.8B to wrong at 6.9B, suggesting capability can be unstable near emergence thresholds.\u003C/li>\n\u003C/ul>\n\u003Ch2 id=\"evaluative-knowledge-can-it-judge-code\">Evaluative Knowledge: Can It Judge Code?\u003C/h2>\n\u003Cp>Knowing definitions is one thing. Can the model identify accessibility problems in code?\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Prompt\u003C/th>\u003Cth>160M\u003C/th>\u003Cth>410M\u003C/th>\u003Cth>1B\u003C/th>\u003Cth>2.8B\u003C/th>\u003Cth>6.9B\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>\u003Ccode>&#x3C;img src='photo.jpg'>\u003C/code> is missing\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>blank\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>”.”\u003C/td>\u003Ctd>❌\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>A \u003Ccode>&#x3C;div>\u003C/code> with onclick is not accessible because\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>⚠️\u003Cbr>\u003Cbr>”not a form control”\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>\u003Ccode>&#x3C;input type='text'>\u003C/code> needs a\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>“password”\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>HTML soup\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>“value”\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>“value”\u003C/td>\u003Ctd>❌\u003Cbr>\u003Cbr>“value”\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>A button that only says ‘Click here’ is bad because\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>❌\u003C/td>\u003Ctd>✅\u003Cbr>\u003Cbr>“not clear what the button does”\u003C/td>\u003Ctd>✅\u003Cbr>\u003Cbr>“not clear what the button does”\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>\u003Cstrong>Findings:\u003C/strong>\u003C/p>\n\u003Cp>There’s a gap between declarative and evaluative knowledge. The 2.8B model can define “alt text” correctly but can’t identify that an \u003Ccode>&#x3C;img>\u003C/code> tag is missing one. However, it \u003Cem>does\u003C/em> correctly identify the problem with “Click here” buttons—ambiguous link text.\u003C/p>\n\u003Cp>At 6.9B, the div/onclick answer (“not a form control”) shows progress—it’s recognizing something about interactivity expectations, even if the explanation isn’t precise.\u003C/p>\n\u003Ch2 id=\"recognition-vs-generation\">Recognition vs. Generation\u003C/h2>\n\u003Cp>Perplexity measures how “surprised” a model is by text. Lower perplexity means the model finds the text more expected. I tested whether models could \u003Cem>recognize\u003C/em> correct definitions even if they couldn’t generate them:\u003C/p>\n\u003Cpre class=\"astro-code astro-code-themes min-light night-owl\" style=\"--shiki-light:#24292eff;--shiki-dark:#d6deeb;--shiki-light-bg:#ffffff;--shiki-dark-bg:#011627; overflow-x: auto;--file-name-offset: -0.75rem;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">correct \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\"> \"\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#ECC48D\">A screen reader is software that reads text aloud for blind users.\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\">\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">wrong \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\"> \"\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#ECC48D\">A screen reader is a device for viewing screens.\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\">\"\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Model\u003C/th>\u003Cth>Correct\u003C/th>\u003Cth>Wrong\u003C/th>\u003Cth>Prefers\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>160M\u003C/td>\u003Ctd>107\u003C/td>\u003Ctd>41\u003C/td>\u003Ctd>Wrong by 2.6x\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>410M\u003C/td>\u003Ctd>40\u003C/td>\u003Ctd>33\u003C/td>\u003Ctd>Wrong by 1.2x\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>1B\u003C/td>\u003Ctd>19\u003C/td>\u003Ctd>42\u003C/td>\u003Ctd>Correct by 2.2x\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>2.8B\u003C/td>\u003Ctd>14\u003C/td>\u003Ctd>55\u003C/td>\u003Ctd>Correct by 4x\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>The preference flips between 410M and 1B. But 1B’s generation was still weak—it said “reads text from a screen,” missing the crucial “aloud.”\u003C/p>\n\u003Cp>Recognition precedes generation. The model knows the right answer before it can produce it.\u003C/p>\n\u003Ch2 id=\"looking-inside-attention-patterns\">Looking Inside: Attention Patterns\u003C/h2>\n\u003Cp>\u003Cstrong>Beyond just testing what models know, TransformerLens lets you see \u003Cem>how\u003C/em> they represent that knowledge internally.\u003C/strong> I looked at how the 2.8B model processes “A screen reader is.”\u003C/p>\n\u003Cp>The question: does the model treat “screen reader” as a compound term, or as two separate words?\u003C/p>\n\u003Cp>Using circuitsvis to visualize attention patterns:\u003C/p>\n\u003Cpre class=\"astro-code astro-code-themes min-light night-owl\" style=\"--shiki-light:#24292eff;--shiki-dark:#d6deeb;--shiki-light-bg:#ffffff;--shiki-dark-bg:#011627; overflow-x: auto;--file-name-offset: -0.75rem;\" tabindex=\"0\" data-language=\"python\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-light-font-style:inherit;--shiki-dark:#C792EA;--shiki-dark-font-style:italic\">import\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\"> circuitsvis \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-light-font-style:inherit;--shiki-dark:#C792EA;--shiki-dark-font-style:italic\">as\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\"> cv\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">prompt \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\"> \"\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#ECC48D\">A screen reader is\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\">\"\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">tokens \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\"> model\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">.\u003C/span>\u003Cspan style=\"--shiki-light:#6F42C1;--shiki-dark:#B2CCD6\">to_str_tokens\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">(\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#82AAFF\">prompt\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">logits\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">,\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\"> cache \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\"> model\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">.\u003C/span>\u003Cspan style=\"--shiki-light:#6F42C1;--shiki-dark:#B2CCD6\">run_with_cache\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">(\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#82AAFF\">prompt\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">)\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">layer \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#1976D2;--shiki-dark:#F78C6C\"> 31\u003C/span>\u003Cspan style=\"--shiki-light:#C2C3C5;--shiki-light-font-style:inherit;--shiki-dark:#637777;--shiki-dark-font-style:italic\">  # last layer\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">attention \u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\"> cache\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">[\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\">\"\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#ECC48D\">pattern\u003C/span>\u003Cspan style=\"--shiki-light:#22863A;--shiki-dark:#D9F5DD\">\"\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">,\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\"> layer\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">]\u003C/span>\u003C/span>\n\u003Cspan class=\"line\">\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">cv\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">.\u003C/span>\u003Cspan style=\"--shiki-light:#24292EFF;--shiki-dark:#D6DEEB\">attention\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">.\u003C/span>\u003Cspan style=\"--shiki-light:#6F42C1;--shiki-dark:#B2CCD6\">attention_patterns\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">(\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D7DBE0\">tokens\u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#82AAFF\">tokens\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D9F5DD\">,\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D7DBE0\"> attention\u003C/span>\u003Cspan style=\"--shiki-light:#D32F2F;--shiki-dark:#C792EA\">=\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#82AAFF\">attention\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">[\u003C/span>\u003Cspan style=\"--shiki-light:#1976D2;--shiki-dark:#F78C6C\">0\u003C/span>\u003Cspan style=\"--shiki-light:#212121;--shiki-dark:#D6DEEB\">])\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>\u003Cstrong>How to read this:\u003C/strong> The visualization shows a grid where each row represents a token in the prompt. The darkness of each cell shows how much attention that token pays to previous tokens. When “reader” has a dark cell under “screen,” it means the model is connecting these words together.\u003C/p>\n\u003Cp>\u003Cimg __ASTRO_IMAGE_=\"{&#x22;src&#x22;:&#x22;../../assets/images/pythia-a11y-emergence-csvis.png&#x22;,&#x22;alt&#x22;:&#x22;Attention pattern visualization showing token \\&#x22;reader\\&#x22; attending strongly to token \\&#x22;screen\\&#x22; in layer 31, head 23 of Pythia 2.8B model.&#x22;,&#x22;title&#x22;:&#x22;A warm and inviting scene of a person using AI-powered tools. The AI presence is subtle yet supportive, symbolizing empowerment and inclusivity.&#x22;,&#x22;index&#x22;:0}\">\u003C/p>\n\u003Cp>At layer 31, Head 23 shows “reader” attending strongly to “screen”—the model binds them as a unit rather than treating them as separate words.\u003C/p>\n\u003Cp>This binding appears across multiple layers:\u003C/p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003Ctable>\u003Cthead>\u003Ctr>\u003Cth>Layer\u003C/th>\u003Cth>Heads showing “reader → screen” binding\u003C/th>\u003C/tr>\u003C/thead>\u003Ctbody>\u003Ctr>\u003Ctd>5\u003C/td>\u003Ctd>23\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>6\u003C/td>\u003Ctd>13\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>10\u003C/td>\u003Ctd>19, 27\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>11\u003C/td>\u003Ctd>29\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>12\u003C/td>\u003Ctd>10, 31\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>14\u003C/td>\u003Ctd>21\u003C/td>\u003C/tr>\u003Ctr>\u003Ctd>31\u003C/td>\u003Ctd>23\u003C/td>\u003C/tr>\u003C/tbody>\u003C/table>\n\u003Cp>The concept gets reinforced through the network rather than computed once. Multiple heads at multiple layers all agree: these tokens belong together.\u003C/p>\n\u003Cp>At 6.9B, even more heads participate in the binding, especially in early-middle layers (5, 7, 10, 11), with less activity in the final layer. The larger model does more distributed, earlier processing.\u003C/p>\n\u003Ch2 id=\"what-this-means\">What This Means\u003C/h2>\n\u003Cp>\u003Cstrong>For accessibility tooling:\u003C/strong> Don’t expect small models to help with accessibility. Even at 1B parameters, knowledge is partial and unreliable. The 2.8B threshold suggests you need substantial model scale before accessibility concepts are meaningfully represented—and even then, evaluative capability lags behind.\u003C/p>\n\u003Cp>\u003Cstrong>For training data:\u003C/strong> The WCAG and ARIA results are telling. WCAG emerges at 6.9B; ARIA never does. These are foundational accessibility acronyms, but they’re rare enough in web-scale training data that emergence requires billions of parameters—or doesn’t happen at all. Specialized training data or fine-tuning would likely be needed for reliable accessibility tooling.\u003C/p>\n\u003Cp>\u003Cstrong>For evaluative tasks:\u003C/strong> The declarative/evaluative gap suggests that even models that “know” accessibility concepts may not reliably apply them to code review. Recognition precedes generation, and generation precedes evaluation. The “Click here” success is promising, but it’s one prompt out of five.\u003C/p>\n\u003Cp>\u003Cstrong>For emergence research:\u003C/strong> Accessibility concepts are a useful probe for studying emergence. They’re concrete enough to test (unlike “reasoning”), rare enough to show scale effects (unlike “Paris is the capital of France”), and have clear right/wrong answers.\u003C/p>\n\u003Ch2 id=\"code\">Code\u003C/h2>\n\u003Cp>Full notebook: \u003Ca href=\"https://github.com/trishasalas/mech-interp-research/blob/main/pythia/pythia-a11y-emergence.ipynb\">https://github.com/trishasalas/mech-interp-research/blob/main/pythia/pythia-a11y-emergence.ipynb\u003C/a>\u003C/p>\n\u003Ch2 id=\"references\">References\u003C/h2>\n\u003Cp>Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O’Brien, K., Hallahan, E., … &#x26; Belrose, N. (2023). Pythia: A suite for analyzing large language models across training and scaling. \u003Cem>arXiv preprint arXiv:2304.01373\u003C/em>.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cem>I’m an accessibility consultant exploring mechanistic interpretability. Find me on LinkedIn or Substack.\u003C/em>\u003C/p>",{"headings":110,"localImagePaths":140,"remoteImagePaths":141,"frontmatter":142,"imagePaths":145},[111,113,116,119,122,125,128,131,134,137],{"depth":112,"slug":94,"text":98},1,{"depth":73,"slug":114,"text":115},"why-pythia","Why Pythia",{"depth":73,"slug":117,"text":118},"setup","Setup",{"depth":73,"slug":120,"text":121},"declarative-knowledge-does-the-model-know-definitions","Declarative Knowledge: Does the Model Know Definitions?",{"depth":73,"slug":123,"text":124},"evaluative-knowledge-can-it-judge-code","Evaluative Knowledge: Can It Judge Code?",{"depth":73,"slug":126,"text":127},"recognition-vs-generation","Recognition vs. Generation",{"depth":73,"slug":129,"text":130},"looking-inside-attention-patterns","Looking Inside: Attention Patterns",{"depth":73,"slug":132,"text":133},"what-this-means","What This Means",{"depth":73,"slug":135,"text":136},"code","Code",{"depth":73,"slug":138,"text":139},"references","References",[105],[],{"pubDatetime":143,"title":98,"featured":99,"draft":17,"tags":144,"description":101},["Date","2026-01-19T00:00:00.000Z"],[19],[105]]